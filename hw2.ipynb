{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2\n",
    "*For any questions or concerns please email your instructor at `smanna@scu.edu`*\n",
    "### Due - 05/04/2018 - 11:59p - 100 points\n",
    "\n",
    "**Name:** `<Alma Niu>`\n",
    "\n",
    "**Email** `<aniu@scu.edu>`\n",
    "\n",
    "**Objectives:** The main aim of this homework is to make you conversant with `Perceptron Training Rule` and use it as a simple binary classifier.\n",
    "\n",
    "**Submission Instructions:** \n",
    "* Please download `hw2.ipynb`, `train.csv`, and `test.csv` files and insert cells in the `hw2.ipynb` to complete your homework and submit the same file to Camino under `Homework` $\\rightarrow$ `hw2`.\n",
    "* Please make sure you type your `Name` and `Email` on top of your submission file in the placeholder above. \n",
    "\n",
    "**Honor Code:** You are expected to complete the homework on your own. Solutions might exist elsewhere, but you are not allowed to copy them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "You are given a dataset of different `Iris` flower species. The original [dataset](https://archive.ics.uci.edu/ml/datasets/iris) has detailed description of all the characteristics of the flowers. Please feel free to visit the original page to know more about the dataset. Your task is to develop a simple linear binary classifier using `perceptron training rule` and `delta rule using batch gradient descent` to categorize the flowers based on their species.\n",
    "\n",
    "**Adaptation**: To make the dataset work for this *homework*, we have modified the original dataset. You can use `train.csv` to train the parameters of your model and then use `test.csv` to test your model on some unknown dataset.\n",
    "\n",
    "**Evaluation**: We have set aside another dataset similar to `test.csv` which will not be disclosed to you. We will use that dataset to evaluate your algorithm. This is just to make sure that you do not have any hard-coded parameters in your code.\n",
    "\n",
    "**Note**: Please make sure you have proper comments and explanation/justification of the steps you have coded. Please feel to add extra cells to complete all the `TODO` stubs. You are allowed to create your own utility functions to support your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define headers since the dataset does not have any headers\n",
    "col_names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.371795</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.628205</td>\n",
       "      <td>0.243590</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.628205</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.628205</td>\n",
       "      <td>0.397436</td>\n",
       "      <td>0.141026</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.371795</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.243590</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  species\n",
       "0      0.551282     0.371795      0.153846     0.012821        1\n",
       "1      0.628205     0.243590      0.435897     0.115385        1\n",
       "2      0.628205     0.410256      0.166667     0.012821        1\n",
       "3      0.628205     0.397436      0.141026     0.012821        1\n",
       "4      0.820513     0.371795      0.653846     0.243590        0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import csv file into pandas dataframe'\n",
    "# TODO\n",
    "df = pd.read_csv('train.csv')\n",
    "df.columns=col_names\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.371795</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.628205</td>\n",
       "      <td>0.243590</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.628205</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.628205</td>\n",
       "      <td>0.397436</td>\n",
       "      <td>0.141026</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.371795</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.243590</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.782051</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.628205</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.910256</td>\n",
       "      <td>0.397436</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>0.217949</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.910256</td>\n",
       "      <td>0.448718</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  species\n",
       "0      0.551282     0.371795      0.153846     0.012821        1\n",
       "1      0.628205     0.243590      0.435897     0.115385        1\n",
       "2      0.628205     0.410256      0.166667     0.012821        1\n",
       "3      0.628205     0.397436      0.141026     0.012821        1\n",
       "4      0.820513     0.371795      0.653846     0.243590        0\n",
       "5      0.782051     0.358974      0.538462     0.153846        1\n",
       "6      0.628205     0.423077      0.192308     0.038462        1\n",
       "7      0.730769     0.333333      0.512821     0.115385        1\n",
       "8      0.910256     0.397436      0.756410     0.217949        0\n",
       "9      0.910256     0.448718      0.769231     0.307692        0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print first 10 rows of the training data\n",
    "# TODO\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the number of training data\n",
    "# TODO\n",
    "np.size(df,0) #0 means count row\n",
    "   #np.size(df,1) output 5, counts total column features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "# check the data types for each columns of your dataset\n",
    "# TODO\n",
    "print(df.sepal_length.dtype)\n",
    "print (df.sepal_width.dtype)\n",
    "print(df.petal_length.dtype)\n",
    "print(df.petal_width.dtype)\n",
    "print(df.species.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.551282  0.371795  0.153846  0.0128205]\n",
      " [0.628205  0.24359   0.435897  0.115385 ]\n",
      " [0.628205  0.410256  0.166667  0.0128205]\n",
      " [0.628205  0.397436  0.141026  0.0128205]\n",
      " [0.820513  0.371795  0.653846  0.24359  ]\n",
      " [0.782051  0.358974  0.538462  0.153846 ]\n",
      " [0.628205  0.423077  0.192308  0.0384615]\n",
      " [0.730769  0.333333  0.512821  0.115385 ]\n",
      " [0.910256  0.397436  0.75641   0.217949 ]\n",
      " [0.910256  0.448718  0.769231  0.307692 ]\n",
      " [0.717949  0.474359  0.205128  0.025641 ]\n",
      " [0.679487  0.487179  0.205128  0.0384615]\n",
      " [0.820513  0.371795  0.692308  0.217949 ]\n",
      " [0.653846  0.333333  0.487179  0.166667 ]\n",
      " [0.717949  0.346154  0.512821  0.153846 ]\n",
      " [0.692308  0.307692  0.5       0.153846 ]\n",
      " [0.846154  0.384615  0.551282  0.166667 ]\n",
      " [0.602564  0.423077  0.192308  0.0128205]\n",
      " [0.653846  0.423077  0.166667  0.0128205]\n",
      " [0.717949  0.307692  0.628205  0.24359  ]\n",
      " [0.602564  0.423077  0.230769  0.0128205]\n",
      " [0.807692  0.384615  0.692308  0.217949 ]\n",
      " [0.615385  0.384615  0.179487  0.       ]\n",
      " [0.615385  0.307692  0.564103  0.205128 ]\n",
      " [0.820513  0.397436  0.641026  0.24359  ]\n",
      " [0.769231  0.358974  0.589744  0.166667 ]\n",
      " [0.602564  0.371795  0.166667  0.025641 ]\n",
      " [0.769231  0.346154  0.5       0.153846 ]\n",
      " [0.705128  0.371795  0.564103  0.179487 ]\n",
      " [0.692308  0.435897  0.153846  0.0128205]\n",
      " [0.846154  0.384615  0.589744  0.179487 ]\n",
      " [0.807692  0.346154  0.705128  0.25641  ]\n",
      " [0.794872  0.423077  0.705128  0.294872 ]\n",
      " [0.589744  0.397436  0.192308  0.0128205]\n",
      " [0.846154  0.384615  0.705128  0.294872 ]\n",
      " [0.692308  0.294872  0.474359  0.128205 ]\n",
      " [0.538462  0.371795  0.128205  0.       ]\n",
      " [0.602564  0.384615  0.192308  0.0128205]\n",
      " [0.846154  0.371795  0.653846  0.282051 ]\n",
      " [1.        0.474359  0.807692  0.24359  ]\n",
      " [0.782051  0.346154  0.602564  0.217949 ]\n",
      " [0.717949  0.320513  0.435897  0.115385 ]\n",
      " [0.692308  0.294872  0.461538  0.115385 ]\n",
      " [0.794872  0.346154  0.641026  0.179487 ]\n",
      " [0.846154  0.371795  0.628205  0.205128 ]\n",
      " [0.628205  0.282051  0.410256  0.115385 ]\n",
      " [0.653846  0.435897  0.179487  0.0128205]\n",
      " [0.884615  0.397436  0.589744  0.166667 ]\n",
      " [0.653846  0.512821  0.179487  0.       ]\n",
      " [0.615385  0.384615  0.179487  0.       ]\n",
      " [0.820513  0.346154  0.576923  0.179487 ]\n",
      " [0.602564  0.371795  0.166667  0.       ]\n",
      " [0.628205  0.371795  0.192308  0.0128205]\n",
      " [0.666667  0.461538  0.179487  0.0128205]\n",
      " [0.705128  0.358974  0.448718  0.153846 ]\n",
      " [0.641026  0.307692  0.371795  0.128205 ]\n",
      " [0.628205  0.435897  0.192308  0.0641026]\n",
      " [0.807692  0.397436  0.666667  0.282051 ]\n",
      " [0.628205  0.423077  0.179487  0.0128205]\n",
      " [0.807692  0.333333  0.666667  0.230769 ]\n",
      " [0.820513  0.371795  0.730769  0.269231 ]\n",
      " [0.641026  0.423077  0.179487  0.0128205]\n",
      " [0.74359   0.371795  0.525641  0.179487 ]\n",
      " [0.730769  0.333333  0.641026  0.230769 ]\n",
      " [0.846154  0.307692  0.730769  0.217949 ]\n",
      " [0.769231  0.371795  0.576923  0.166667 ]\n",
      " [0.794872  0.307692  0.628205  0.230769 ]\n",
      " [0.807692  0.358974  0.538462  0.153846 ]\n",
      " [0.641026  0.474359  0.192308  0.0128205]\n",
      " [0.692308  0.320513  0.551282  0.141026 ]\n",
      " [0.74359   0.397436  0.602564  0.217949 ]\n",
      " [0.794872  0.358974  0.705128  0.217949 ]\n",
      " [0.923077  0.358974  0.794872  0.217949 ]\n",
      " [0.871795  0.384615  0.615385  0.179487 ]\n",
      " [0.628205  0.435897  0.153846  0.025641 ]\n",
      " [0.897436  0.371795  0.74359   0.25641  ]\n",
      " [0.75641   0.269231  0.5       0.115385 ]\n",
      " [0.961538  0.371795  0.833333  0.25641  ]\n",
      " [0.641026  0.461538  0.179487  0.0384615]\n",
      " [0.807692  0.346154  0.705128  0.269231 ]\n",
      " [0.717949  0.346154  0.564103  0.153846 ]\n",
      " [0.782051  0.269231  0.564103  0.179487 ]\n",
      " [0.782051  0.423077  0.679487  0.282051 ]\n",
      " [0.794872  0.333333  0.615385  0.217949 ]\n",
      " [0.769231  0.320513  0.705128  0.166667 ]\n",
      " [0.730769  0.333333  0.641026  0.230769 ]\n",
      " [0.679487  0.423077  0.179487  0.0384615]\n",
      " [0.692308  0.525641  0.166667  0.0128205]\n",
      " [0.589744  0.397436  0.153846  0.0128205]\n",
      " [0.833333  0.371795  0.551282  0.166667 ]\n",
      " [0.564103  0.282051  0.153846  0.025641 ]\n",
      " [0.858974  0.397436  0.74359   0.282051 ]\n",
      " [0.615385  0.384615  0.179487  0.       ]\n",
      " [0.692308  0.282051  0.5       0.153846 ]\n",
      " [0.641026  0.410256  0.205128  0.0512821]\n",
      " [0.615385  0.371795  0.166667  0.0128205]\n",
      " [0.679487  0.423077  0.205128  0.0128205]\n",
      " [0.679487  0.371795  0.564103  0.179487 ]\n",
      " [0.974359  0.320513  0.871795  0.282051 ]\n",
      " [0.846154  0.410256  0.717949  0.307692 ]\n",
      " [0.74359   0.371795  0.641026  0.217949 ]\n",
      " [0.641026  0.474359  0.179487  0.025641 ]\n",
      " [0.730769  0.5       0.141026  0.0128205]\n",
      " [0.730769  0.346154  0.641026  0.294872 ]\n",
      " [0.628205  0.448718  0.166667  0.0128205]\n",
      " [0.576923  0.397436  0.166667  0.0128205]\n",
      " [0.576923  0.423077  0.166667  0.025641 ]\n",
      " [0.871795  0.384615  0.679487  0.25641  ]\n",
      " [0.717949  0.371795  0.525641  0.141026 ]\n",
      " [0.75641   0.423077  0.564103  0.192308 ]\n",
      " [0.974359  0.346154  0.846154  0.24359  ]]\n"
     ]
    }
   ],
   "source": [
    "# Separating features and store in X_train\n",
    "# TODO\n",
    "X_train=df.loc[:,'sepal_length':'petal_width'].as_matrix()\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# Assigning y_train (species) from the dataset\n",
    "# TODO\n",
    "y_train=df.loc[:,['species']].as_matrix()\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the number of y values in the training dataset\n",
    "# TODO\n",
    "len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation functions**: You have to implement two different activation functions, `sgn` and `sigmoid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of sign activation\n",
    "def sgn(x): # i\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of sigmoid activation\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Rules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of perceptron training algorithm\n",
    "def PTR(x, y, num_iter, learning_rate, activation_func):\n",
    "    # TODO\n",
    "    W=np.zeros(4,np.float64) #equal number of features,initialize weight vector \n",
    "    b=np.zeros(1,np.float64) #0 error\n",
    "    for t in range(num_iter):\n",
    "        i=t%len(y)\n",
    "        y_pred=activation_func(np.dot(W,x[i].T)+b) #work after evaluating dot product\n",
    "        if y_pred!=y[i]:\n",
    "            W=W + (learning_rate*(y[i]-y_pred)* x[i,:]) #put in activation func\n",
    "            b=b + learning_rate* (y[i]-y_pred)\n",
    "    return W,b\n",
    "    #return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of delta rule using batch gradient decent\n",
    "\n",
    "#convert X_train and y_train to matrix first\n",
    "def BGD(X_train, y_train, num_iter, learning_rate):\n",
    "#     TODO\n",
    "\n",
    "    W=np.ones(X_train.shape[1])\n",
    "    cost=np.zeros(num_iter)\n",
    "    weights=np.ones(X_train.shape[1]) #weights\n",
    "    b=0 #error\n",
    "    for t in range(num_iter):\n",
    "        O=X_train.dot(W) +b #cross multiply all features\n",
    "        for i in range(len(O)):\n",
    "            weights=weights+(learning_rate*(y_train[i]-O[i])*X_train[i]) #EQ1\n",
    "            b=b+(learning_rate* (y_train[i]-O[i]))\n",
    "            cost[t]=cost[t]+(y_train[i]-O[i])**2\n",
    "    W=W+weights #EQ2\n",
    "    return W,b,cost \n",
    "#    return W, b, cost \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning parameters using Batch Gradient Descent for training\n",
    "W_bgd, b_bgd, cost_bgd = BGD(X_train, y_train, 100, 0.01) # try out with different num_iterations, and learning_rate\n",
    "\n",
    "#worked, need to set X_train & y_train as matrix form!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4d107d0690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot cost (as computed in batch gradient descent)\n",
    "# with `No. of iterations` in x-axis, and `Cost` in y-axis\n",
    "# TODO\n",
    "# plot the cost\n",
    "plt.plot(cost_bgd)\n",
    "plt.xlabel(\"# iterations\")\n",
    "plt.ylabel(\"cost\")\n",
    "plt.show()\n",
    "#plotLearning2D(y_train, X_train, W_bgd, b_bgd) #adjust num interations to 100 for smooth curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning parameters using Perceptron training rule function\n",
    "W_ptr, b_ptr = PTR(X_train, y_train, 2500, 0.01, sigmoid) # try out with different num_iterations, and learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction Begins**: I have provided you with two utility functions `Identity` and `predict` which you will need to use it on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An activation, that does nothing. Used to disable activation_function.\n",
    "def Identity(x):\n",
    "    return x\n",
    "\n",
    "# The neural network thinks.\n",
    "def predict(x_test, weights, bias, activation_func=Identity):\n",
    "    return activation_func(np.dot(x_test, weights) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test data into pandas dataframe\n",
    "# TODO\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_test.columns=col_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.474359</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>0.525641</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.756410</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.320513</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.141026</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.858974</td>\n",
       "      <td>0.371795</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.256410</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.294872</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.705128</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.243590</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  species\n",
       "0      0.794872     0.307692      0.615385     0.179487        1\n",
       "1      0.794872     0.282051      0.551282     0.153846        1\n",
       "2      0.974359     0.474359      0.846154     0.269231        0\n",
       "3      0.717949     0.358974      0.525641     0.153846        1\n",
       "4      0.756410     0.333333      0.641026     0.192308        1\n",
       "5      0.730769     0.320513      0.500000     0.141026        1\n",
       "6      0.858974     0.371795      0.692308     0.256410        0\n",
       "7      0.615385     0.294872      0.410256     0.115385        1\n",
       "8      0.705128     0.346154      0.615385     0.243590        0\n",
       "9      0.576923     0.384615      0.179487     0.012821        1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print first 10 rows of the test data\n",
    "# TODO\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.794872  0.307692  0.615385  0.179487 ]\n",
      " [0.794872  0.282051  0.551282  0.153846 ]\n",
      " [0.974359  0.474359  0.846154  0.269231 ]\n",
      " [0.717949  0.358974  0.525641  0.153846 ]\n",
      " [0.75641   0.333333  0.641026  0.192308 ]\n",
      " [0.730769  0.320513  0.5       0.141026 ]\n",
      " [0.858974  0.371795  0.692308  0.25641  ]\n",
      " [0.615385  0.294872  0.410256  0.115385 ]\n",
      " [0.705128  0.346154  0.615385  0.24359  ]\n",
      " [0.576923  0.384615  0.179487  0.0128205]\n",
      " [0.705128  0.371795  0.512821  0.153846 ]\n",
      " [0.705128  0.307692  0.487179  0.128205 ]\n",
      " [0.871795  0.397436  0.717949  0.282051 ]\n",
      " [0.807692  0.397436  0.564103  0.179487 ]\n",
      " [0.935897  0.346154  0.769231  0.230769 ]\n",
      " [0.769231  0.371795  0.615385  0.217949 ]\n",
      " [0.641026  0.435897  0.166667  0.025641 ]\n",
      " [0.705128  0.333333  0.525641  0.153846 ]]\n"
     ]
    }
   ],
   "source": [
    "# features of test\n",
    "# Separating feature and output columns from the dataset and store in X_test\n",
    "# TODO\n",
    "X_test=df_test.loc[:,'sepal_length':'petal_width'].as_matrix()\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# store actual output (target) into y_test\n",
    "# TODO\n",
    "y_test=df_test.loc[:,['species']].as_matrix()\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions with X_test\n",
    "y_pred_ptr = predict(X_test, W_ptr, b_ptr, sigmoid) # feel free to explore results using `sgn` activation function\n",
    "y_pred_bgd = predict(X_test, W_bgd, b_bgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation Metrics** We will evaluate our predictions using `Root Mean Squared Error`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using RMSE\n",
    "def RMSE(target, output):\n",
    "    return math.sqrt(((target - output) ** 2).mean(axis=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the rmse is: ', 0.47788783586090405)\n"
     ]
    }
   ],
   "source": [
    "rmse_ptr = RMSE(y_test, y_pred_ptr)\n",
    "print('the rmse is: ', rmse_ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the rmse is: ', 17.000442411075863)\n"
     ]
    }
   ],
   "source": [
    "rmse_bgd = RMSE(y_test, y_pred_bgd)\n",
    "print('the rmse is: ', rmse_bgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
