{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3\n",
    "*For any questions or concerns please email your instructor at `smanna@scu.edu`*\n",
    "### Due - 05/21/2018 - 11:59p - 100 points\n",
    "\n",
    "**Name:** `<Alma Niu>`\n",
    "\n",
    "**Email** `<aniu@scu.edu>`\n",
    "\n",
    "**Objectives:** \n",
    "* Make you conversant with `Multi-layer Perceptrons` (MLP)\n",
    "* Implement and understand multi-class classifier using MLP\n",
    "* Familiarity with `TensorFlow`\n",
    "\n",
    "**Submission Instructions:** \n",
    "* Please download `hw3.ipynb`, `train_nn.csv`, and `test_nn.csv` files and insert cells in the `hw3.ipynb` to complete your homework and submit the same file to Camino under `Homework` $\\rightarrow$ `hw3`.\n",
    "* Please make sure you type your `Name` and `Email` on top of your submission file in the placeholder above. \n",
    "\n",
    "**Honor Code:** You are expected to complete the homework on your own. Solutions might exist elsewhere, but you are not allowed to copy them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "You are given a dataset of different `Iris` flower species. The original [dataset](https://archive.ics.uci.edu/ml/datasets/iris) has detailed description of all the characteristics of the flowers. Please feel free to visit the original page to know more about the dataset. Your task is to develop a Neural Network based multi-class classifier using TensorFlow to categorize the flowers based on their species. \n",
    "\n",
    "**Guide**: Feel free to use `week6-MLP-binaryData-tf.ipynb` as a sample to kick-start your implentation. If you simply try to copy and paste the code into your homework, trust me, it won't work!!! \n",
    "\n",
    "Here are few things you can try:\n",
    "* *One hot encoding*: There are three `species` of iris flower in your dataset. This time, you have to classify your data into `three` different categories. For that you need to compute `one hot encoding`.\n",
    "* *Activation functions*: There are different activation functions used in Neural Network (NN). Your task is to research different activation functions such as SoftMax, ReLu, and Sigmoid, and learn which ones are suitable for which layers of your neural network. Based on that, design your NN architecture and check the performance by computing `accuracy`.\n",
    "* *Number of hidden layers*: Instead on using one hidden layer, you can try two and check out your results and check the performance by computing `accuracy`.\n",
    "* *Loss functions*: TensorFlow comes with different forms of `loss` functions. Your task is to review them and pick any two to report your findings.\n",
    "\n",
    "**Adaptation**: We have provided you with the original dataset splitting it into two, one for training and the other for testing. You can use `train_nn.csv` to train the parameters of your model and then use `test_nn.csv` to test your model.\n",
    "\n",
    "**Evaluation**: We have set aside another dataset similar to `test_nn.csv` which will not be disclosed to you. We will use that dataset to evaluate your algorithm. This is just to make sure that you do not have any hard-coded parameters in your code.\n",
    "\n",
    "**Note**: Please make sure you have proper comments and explanation/justification of the steps you have coded. Please feel to add extra cells to complete all the `TODO` stubs. You are allowed to create your own utility functions to support your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# TODO - feel free to add more as you need!\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sepal_length  sepal_width  petal_length  petal_width  species\n",
      "6            4.4          3.2           1.3          0.2      1.0\n",
      "34           6.7          3.0           5.2          2.3      0.0\n",
      "35           6.3          2.5           5.0          1.9      0.0\n",
      "8            5.1          3.8           1.9          0.4      1.0\n",
      "3            5.1          3.4           1.5          0.2      1.0\n",
      "18           6.1          3.0           4.6          1.4      2.0\n",
      "25           5.1          2.5           3.0          1.1      2.0\n",
      "29           6.7          3.1           5.6          2.4      0.0\n",
      "19           5.8          2.6           4.0          1.2      2.0\n",
      "1            4.9          3.1           1.5          0.1      1.0\n",
      "31           5.8          2.7           5.1          1.9      0.0\n",
      "13           5.0          3.3           1.4          0.2      1.0\n",
      "9            4.8          3.0           1.4          0.3      1.0\n",
      "2            4.4          3.0           1.3          0.2      1.0\n",
      "28           6.9          3.1           5.4          2.1      0.0\n",
      "24           6.2          2.9           4.3          1.3      2.0\n",
      "23           5.7          2.9           4.2          1.3      2.0\n",
      "22           5.7          3.0           4.2          1.2      2.0\n",
      "30           6.9          3.1           5.1          2.3      0.0\n",
      "27           6.0          3.0           4.8          1.8      0.0\n",
      "33           6.7          3.3           5.7          2.5      0.0\n",
      "11           4.6          3.2           1.4          0.2      1.0\n",
      "32           6.8          3.2           5.9          2.3      0.0\n",
      "10           5.1          3.8           1.6          0.2      1.0\n",
      "14           6.3          2.3           4.4          1.3      2.0\n",
      "12           5.3          3.7           1.5          0.2      1.0\n",
      "4            5.0          3.5           1.3          0.3      1.0\n",
      "26           5.7          2.8           4.1          1.3      2.0\n",
      "37           6.2          3.4           5.4          2.3      0.0\n",
      "16           5.5          2.5           4.0          1.3      2.0\n",
      "5            4.5          2.3           1.3          0.3      1.0\n",
      "36           6.5          3.0           5.2          2.0      0.0\n",
      "7            5.0          3.5           1.6          0.6      1.0\n",
      "17           5.5          2.6           4.4          1.2      2.0\n",
      "21           5.6          2.7           4.2          1.3      2.0\n",
      "38           5.9          3.0           5.1          1.8      0.0\n",
      "15           5.6          3.0           4.1          1.3      2.0\n",
      "20           5.0          2.3           3.3          1.0      2.0\n"
     ]
    }
   ],
   "source": [
    "'Loading the train Iris dataset'\n",
    "# Define headers since the dataset does not have any headers\n",
    "col_names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species\"]\n",
    "# reading the training data into the data frame\n",
    "\n",
    "\"\"\"Note: I mapped species' name to numbers\"\"\"\n",
    "\n",
    "train_df = pd.read_csv('train_nn.csv', header=None, names=col_names)\n",
    "train_df['species']=train_df.species.map({'Iris-virginica':0.0,'Iris-setosa':1.0,'Iris-versicolor':2.0})\n",
    "\n",
    "# reading the test data into the data frame\n",
    "test_df = pd.read_csv('test_nn.csv', header=None, names=col_names)\n",
    "test_df['species']=test_df.species.map({'Iris-virginica':0.0,'Iris-setosa':1.0,'Iris-versicolor':2.0})\n",
    "\n",
    "print(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "'Perform One Hot Encoding for the classifier to work'\n",
    "# TODO\n",
    "Y_train=np.asarray(train_df.species).reshape(-1,1) #make it 2d array(reshape goes up dimension)\n",
    "\n",
    "#print(Y_train)\n",
    "encoder=OneHotEncoder(sparse=False)\n",
    "\n",
    "Y_train=encoder.fit_transform(Y_train) #onehot for training\n",
    "\n",
    "Y_test=np.asarray(test_df.species).reshape(-1,1)\n",
    "Y_test=encoder.fit_transform(Y_test)\n",
    "\n",
    "print(Y_test) #works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.4 3.2 1.3 0.2]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.  2.3 3.3 1. ]]\n"
     ]
    }
   ],
   "source": [
    "'Assigning features and output columns and converting them into numpy arrays'\n",
    "# this is upto you.\n",
    "# you can do it in your way\n",
    "# TODO (optional)\n",
    "\n",
    "X_train=train_df.loc[:,'sepal_length':'petal_width'].as_matrix()\n",
    "\n",
    "X_test=test_df.loc[:,'sepal_length':'petal_width'].as_matrix()\n",
    "print(X_test) #convert into numpy array\n",
    "#Y_train, 2d array will each array inside representing # from one hot encoding\n",
    "#Y_test, same as Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Function for Multilayer perceptron'''\n",
    "\n",
    "def MLP(input_x, weights, biases): # with one hidden layer\n",
    "    layer_1=tf.add(tf.matmul(input_x,weights['w_h']),bias['b_h1'])\n",
    "    layer_1=tf.nn.relu(layer_1)\n",
    "    out_layer=tf.add(tf.matmul(layer_1,weights['w_out']),bias['b_out'])\n",
    "    out_layer=tf.nn.softmax(out_layer) #use softmax for output later\n",
    "    return out_layer\n",
    "# TODO\n",
    "#    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 4)\n",
      "(38, 4)\n",
      "(112, 3)\n",
      "(38, 3)\n"
     ]
    }
   ],
   "source": [
    "# Printing the dimensions of your train, and test data\n",
    "# TODO\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Define the parameters for your neural network such as input_neurons, etc'\n",
    "# TODO\n",
    "n_features=len(X_train[0]) #4 features\n",
    "input_num=len(X_train) #112 inputs for X's datapoints\n",
    "num_hidden_layers=1 # go with 1 hidden layer as my base case \n",
    "num_output=len(Y_test[0])# 3 different category output\n",
    "\n",
    "num_iter = 2000\n",
    "display_step = int(num_iter / 10)\n",
    "learning_rate = 0.009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Initialize placeholders for the nn'\n",
    "#TODO\n",
    "X=tf.placeholder(\"float\",[None,4])\n",
    "Y=tf.placeholder(\"float\",[None,3])\n",
    "'weights and biases'\n",
    "# TODO (You can do it in your way!)\n",
    "\n",
    "#I assigned random weights myself\n",
    "weights={\n",
    "    'w_h':tf.Variable(tf.random_normal([4, 1])), #4 features, 1 hidden \n",
    "    'w_out':tf.Variable(tf.random_normal([1, 3]))#number hidden, number output\n",
    "}\n",
    "\n",
    "bias={\n",
    "    'b_h1' : tf.Variable(tf.zeros([num_hidden_layers])),\n",
    "    'b_out': tf.Variable(tf.zeros([num_output]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss= 0.21969\n",
      "loss= 0.15112\n",
      "loss= 0.14018\n",
      "loss= 0.13151\n",
      "loss= 0.12533\n",
      "loss= 0.12046\n",
      "loss= 0.11631\n",
      "loss= 0.11239\n",
      "loss= 0.10874\n",
      "loss= 0.10427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Store the cost (or loss), so that you can use it to plot the graph'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Defining the model'\n",
    "# calling the MLP function and storing it into an appropriate variable\n",
    "# TODO\n",
    "model=MLP(X, weights,bias)\n",
    "\n",
    "'Defining the cost function'\n",
    "# TODO\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model,labels=Y))\n",
    "'Using gradient descent optimizer for opimization'\n",
    "# TODO\n",
    "# You need to use gradient descent for optimization\n",
    "# try with different learning rate, like 0.01, 0.002, 0.005 etc\n",
    "loss_func = tf.reduce_mean(tf.square(Y - model))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.0085).minimize(cost) #pass in cost as param\n",
    "\n",
    "\n",
    "'Creating a session'\n",
    "# TODO\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'Set the number of epochs'\n",
    "# TODO\n",
    "training_epochs=2000#start minimizing errors\n",
    "\n",
    "errors = []\n",
    "iter = []\n",
    "for k in range(training_epochs):\n",
    "    tmp_cost, _ = sess.run([loss_func, optimizer], feed_dict={X: X_train, Y: Y_train})\n",
    "    if k % display_step == 0:\n",
    "        #print('output: ', sess.run(model, feed_dict={X:X_train}))\n",
    "        errors.append(tmp_cost)\n",
    "        iter.append(k)\n",
    "        print('loss= ' + \"{:.5f}\".format(tmp_cost))\n",
    "\n",
    "# separates the input space\n",
    "\n",
    "'Initialize tensorflow variables'\n",
    "# TODO\n",
    "\n",
    "'Train your model...'\n",
    "'Store the cost (or loss), so that you can use it to plot the graph'\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd964de3710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'Plot loss'\n",
    "# TODO\n",
    "\n",
    "# A sample code is provided below. \n",
    "\n",
    "plt.plot(errors,iter)\n",
    "plt.xlabel(\"No. of iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 1.740764  , -0.22992867, -1.6974922 , -1.1819285 ], dtype=float32), array([-0.23790093,  1.9249266 ,  1.1626838 ], dtype=float32))\n",
      "\n",
      "\n",
      "(array(0.05837217, dtype=float32), array([ 0.7617917 , -0.94213563,  0.18034434], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "'Print weights'\n",
    "# TODO\n",
    "W = np.squeeze(sess.run(weights['w_h']))   # 2x2\n",
    "\n",
    "W2=np.squeeze(sess.run(weights['w_out']))\n",
    "print(W,W2)\n",
    "print('\\n')\n",
    "'Print biases'\n",
    "# TODO\n",
    "b = np.squeeze(sess.run(bias['b_h1']))    # 2,\n",
    "b2=np.squeeze(sess.run(bias['b_out']))\n",
    "print(b,b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38, 3)\n"
     ]
    }
   ],
   "source": [
    "'Test the neural network'\n",
    "# Here you have to find out predicted output for every data from your test dataset\n",
    "# TODO\n",
    "\n",
    "#print y_test.shape\n",
    "X_test.shape #got it, need to reshape X_test in order to work\n",
    "print(Y_test.shape)\n",
    "\n",
    "y_pred = []\n",
    "for i in range(len(X_test)):\n",
    "    y_pred.append(np.rint(sess.run(model, feed_dict={X:[X_test[i]]})))\n",
    "    \n",
    "\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Actual:', array([[0., 1., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 1., 0.],\n",
      "       [0., 1., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 1., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 1., 0.],\n",
      "       [0., 1., 0.],\n",
      "       [0., 1., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 1., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 1., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 1., 0.],\n",
      "       [0., 1., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 1., 0.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 1., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.],\n",
      "       [1., 0., 0.],\n",
      "       [0., 0., 1.],\n",
      "       [0., 0., 1.]]), 'Predicted:', [array([[0., 1., 0.]], dtype=float32), array([[1., 0., 0.]], dtype=float32), array([[1., 0., 0.]], dtype=float32), array([[0., 1., 0.]], dtype=float32), array([[0., 1., 0.]], dtype=float32), array([[0., 0., 0.]], dtype=float32), array([[0., 1., 0.]], dtype=float32), array([[1., 0., 0.]], dtype=float32), array([[0., 0., 0.]], dtype=float32), array([[0., 1., 0.]], dtype=float32), array([[1., 0., 0.]], dtype=float32), array([[0., 1., 0.]], dtype=float32), array([[0., 1., 0.]], dtype=float32), array([[0., 1., 0.]], dtype=float32), array([[1., 0., 0.]], dtype=float32), array([[0., 0., 0.]], dtype=float32), array([[0., 0., 0.]], dtype=float32), array([[0., 0., 0.]], dtype=float32), array([[1., 0., 0.]], dtype=float32), array([[1., 0., 0.]], dtype=float32), array([[1., 0., 0.]], dtype=float32), array([[0., 1., 0.]], dtype=float32), array([[1., 0., 0.]], dtype=float32), array([[0., 1., 0.]], dtype=float32), array([[0., 0., 0.]], dtype=float32), array([[0., 1., 0.]], dtype=float32), array([[0., 1., 0.]], dtype=float32), array([[0., 0., 0.]], dtype=float32), array([[1., 0., 0.]], dtype=float32), array([[0., 0., 0.]], dtype=float32), array([[0., 1., 0.]], dtype=float32), array([[1., 0., 0.]], dtype=float32), array([[0., 1., 0.]], dtype=float32), array([[1., 0., 0.]], dtype=float32), array([[0., 0., 0.]], dtype=float32), array([[1., 0., 0.]], dtype=float32), array([[0., 0., 0.]], dtype=float32), array([[0., 0., 0.]], dtype=float32)])\n"
     ]
    }
   ],
   "source": [
    "# You can print your actual y from test and predicted y using test\n",
    "# you might have to check the dimensions of each to make sure you can compare them later\n",
    "# TODO\n",
    "\n",
    "print('Actual:', Y_test, 'Predicted:', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6578947368421053"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Compute accuracy'\n",
    "# You can use sci-kit learn's accuracy score to evaluate the performance of your model on test data\n",
    "# TODO\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred=np.asarray(y_pred)\n",
    "y_pred.shape\n",
    "accuracy_score(Y_test, np.squeeze(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
